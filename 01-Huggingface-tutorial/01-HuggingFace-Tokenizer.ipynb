{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff7d174",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "```\n",
    "pip install tokenizers\n",
    "wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
    "unzip wikitext-103-raw-v1.zip\n",
    "rm wikitext-103-raw-v1.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f615e78",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "## Training Tokenizer\n",
    "\n",
    " - Byte-Pair Encoding (BPE) Tokenizer Î•º ÌïôÏäµ\n",
    " - speical_tokens: ÏàúÏÑúÍ∞Ä Ï§ëÏöî. `[UNK]` Îäî 0, `[CLS]`Îäî 1 ÏùÑ Í∞ÅÍ∞Å IDÎ°ú Î∂ÄÏó¨Îê®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1a29b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]', vocab_size=30000))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]'])\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# Save & Load\n",
    "# tokenizer.save(\"tokenizer-wiki.json\")\n",
    "# tokenizer = Tokenizer.from_file(\"data/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "902018b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']\n",
      "ids   : [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n",
      "[(0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), (14, 17), (18, 21), (22, 25), (26, 27), (28, 29)]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print('tokens:', output.tokens)\n",
    "print('ids   :', output.ids)\n",
    "print(output.offsets[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80afac97",
   "metadata": {},
   "source": [
    "## Post Processing \n",
    "\n",
    "ÏïÑÎûò ÏΩîÎìúÎäî traditional BERT InputÏúºÎ°ú Î≥ÄÌôò ÌïòÎäî ÏΩîÎìú. <br>\n",
    "\n",
    " - `$A` in single : sentence Î•º ÏùòÎØ∏\n",
    " - `$A` and `$B` in pair : Ï≤´Î≤àÏß∏ Í∑∏Î¶¨Í≥† ÎëêÎ≤àÏß∏ sentences Î•º ÏùòÎØ∏\n",
    " - `:1` in pair : type IDÎ•º ÏùòÎØ∏ÌïòÎ©∞, Í∏∞Î≥∏Í∞íÏúºÎ°ú `:0` Ïù¥ ÏûàÏùå (Îî∞ÎùºÏÑú `$A:0` Í∞Ä Î™ÖÏãúÎêòÏñ¥ ÏûàÏßÄ ÏïäÏùå)\n",
    " \n",
    " \n",
    " encode Ìï¥ÏÑú ÏúÑÏùò ÏòàÏ†úÏôÄ ÎπÑÍµêÌïòÎ©¥ Ïñ¥Îñ§ ÏùòÎØ∏Ïù∏ÏßÄ ÏâΩÍ≤å ÏïåÍ≤å Îê®\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19aabe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e474ab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single:  ['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "Pair  :  ['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'How', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "TypeID:  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Single\n",
    "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print('Single: ', output.tokens)\n",
    "\n",
    "# Pair\n",
    "output = tokenizer.encode(\"Hello, y'all!\", \"How are you üòÅ ?\")\n",
    "print('Pair  : ', output.tokens)\n",
    "print('TypeID: ', output.type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65807bbd",
   "metadata": {},
   "source": [
    "## Encoding Multiple Sentences in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "555afafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', '[SEP]']\n",
      "1: ['[CLS]', 'How', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Single\n",
    "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you üòÅ ?\"])\n",
    "\n",
    "for i, o in enumerate(output):\n",
    "    print(f'{i}:', o.tokens)\n",
    "\n",
    "\n",
    "# Pair\n",
    "output = tokenizer.encode_batch(\n",
    "    [[\"Hello, y'all!\", \"How are you üòÅ ?\"], \n",
    "     [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432ec7f",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    " - `length`: ÏÑ§Ï†ï ÏïàÌïòÎ©¥ sentenceÏ§ëÏóê Í∞ÄÏû• Í∏¥ Í∏∏Ïù¥Î°ú ÏÑ§Ï†ïÎê®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38d07961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Tokens: ['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.enable_padding(pad_id=3, pad_token='[PAD]', length=12)\n",
    "output = tokenizer.encode(\"Hello, y'all!\")\n",
    "print('Padding Tokens:', output.tokens)\n",
    "print('Attention Mask:', output.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041ece9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd02143c",
   "metadata": {},
   "source": [
    "## Pretrained Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35ed2df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-11 00:18:46--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.93.142\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.93.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 231508 (226K) [text/plain]\n",
      "Saving to: ‚Äòbert-base-uncased-vocab.txt‚Äô\n",
      "\n",
      "bert-base-uncased-v 100%[===================>] 226.08K   388KB/s    in 0.6s    \n",
      "\n",
      "2021-06-11 00:18:48 (388 KB/s) - ‚Äòbert-base-uncased-vocab.txt‚Äô saved [231508/231508]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0493c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
    "print(tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\").tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
